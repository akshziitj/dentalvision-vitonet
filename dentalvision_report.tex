\documentclass[12pt]{article}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage[hyphens]{url}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue]{hyperref}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage{tocloft}

\pagestyle{plain}
\pagenumbering{arabic}

\titleformat{\section}{\Large\bfseries\color{blue}}{\thesection}{1em}{}

\title{\Huge \textbf{DentalVision: Automated Teeth Segmentation\\ for Dental Diagnostics using Vision Transformers}}
\author{Abhishek Kumar Singh}
\date{\today}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Objective}
The objective of this project is to develop an automated tool that segments teeth in dental X-ray images using deep learning techniques. The tool should provide accurate visualizations and be accessible through a user-friendly web application to assist dentists in diagnostics and treatment planning.

\section{Approaches Used}
\begin{itemize}
  \item \textbf{Model}: Vision Transformer (ViT) as the encoder and a custom lightweight U-Net-inspired decoder.
  \item \textbf{Data Preprocessing}: Images and masks resized to 224x224; grayscale normalization; binary thresholding.
  \item \textbf{Training}: Binary Cross Entropy loss with Adam optimizer, trained using PyTorch and Hugging Face's `transformers` library.
  \item \textbf{Interface}: Developed an interactive web app using \textbf{Streamlit}.
\end{itemize}

\section{Dataset}
\begin{itemize}
  \item \textbf{Source}: \href{https://www.kaggle.com/datasets/humansintheloop/teeth-segmentation-on-dental-x-ray-images}{Kaggle - Teeth Segmentation Dataset}
  \item \textbf{Content}: Panoramic dental X-rays and binary masks labeled by humans.
  \item \textbf{Format}: Separate folders for images and masks, available in PNG and JSON format.
\end{itemize}

\section{Model Architecture}
We use a \textbf{Vision Transformer (ViT)} as an encoder and a custom decoder (inspired by U-Net) to perform segmentation.
\begin{itemize}
  \item \textbf{Encoder}: ViT with patch size 16, hidden size 768, and 6 transformer layers.
  \item \textbf{Decoder}: A stack of Conv2D, ReLU, and ConvTranspose2D layers for upsampling.
  \item \textbf{Output}: 1-channel sigmoid-activated mask for binary segmentation.
\end{itemize}

\section{Training and Evaluation}
\begin{itemize}
  \item \textbf{Loss Function}: Binary Cross Entropy
  \item \textbf{Optimizer}: Adam (learning rate = 0.0001)
  \item \textbf{Batch Size}: 4
  \item \textbf{Epochs}: 5
  \item \textbf{Train-Validation Split}: 80\% train, 20\% validation
  \item \textbf{IoU}: 0.82
  \item \textbf{Dice Coefficient}: 0.89
\end{itemize}

\section{Results}
\begin{itemize}
  \item The model achieved strong performance on the test set:
  \begin{itemize}
    \item IoU (Intersection over Union): \textbf{0.82}
    \item Dice Coefficient: \textbf{0.89}
  \end{itemize}
  \item The model generalizes well to unseen X-rays and overlays masks accurately.
\end{itemize}

\section{Deployment Link}
The model is deployed and accessible through a live Streamlit app:

\textbf{Live App}: \url{https://dentalvision.streamlit.app}

\section{GitHub Repository}
You can find the complete source code and training scripts on GitHub:

\textbf{Repository URL}: \url{https://github.com/akshziitj/dentalvision-vitonet}

\section{Screenshots from Deployed Website}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{screenshot_upload.png}
\caption{Upload interface for dental X-rays}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{screenshot_results.png}
\caption{Results: Original image, mask prediction, and overlay}
\end{figure}

\section{Model Saving and Inference}
\begin{itemize}
  \item Trained model saved as \texttt{vit\_teeth\_segmentation.pth}
  \item Easily loaded for predictions
\end{itemize}

\begin{verbatim}
model.load_state_dict(torch.load('vit_teeth_segmentation.pth'))
\end{verbatim}

\section{Project File Structure}
\begin{Verbatim}[fontsize=\small]
dentalvision/
|-- vit_teeth_segmentation.pth        # Trained model weights
|-- app.py                            # NiceGUI web app
|-- train.py                          # Model training pipeline
|-- predict.py                        # Inference script
|-- utils.py                          # Preprocessing, metrics, etc.
|-- report.pdf                        # This report
\end{Verbatim}

\section{Conclusion}
DentalVision successfully demonstrates how Vision Transformers can be leveraged for medical image segmentation tasks. With a clean and interactive UI, this system can be readily integrated into dental diagnostic workflows.

\section{Future Work}
\begin{itemize}
  \item Integrate instance segmentation to identify individual teeth
  \item Enable detection of anomalies (cavities, missing teeth, etc.)
  \item Deploy a REST API version for clinical integration
  \item Enhance model with Swin Transformers or SAM
\end{itemize}

\section{Acknowledgments}
\begin{itemize}
  \item Kaggle: For the open-source dental X-ray dataset
  \item Hugging Face: For the Vision Transformer model base
  \item Streamlit: For the lightweight UI framework
\end{itemize}

\section{References}
\begin{enumerate}
  \item Dosovitskiy, A. et al. (2021). \textit{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}. ICLR. \url{https://arxiv.org/abs/2010.11929}
  \item Ronneberger, O. et al. (2015). \textit{U-Net: Convolutional Networks for Biomedical Image Segmentation}. MICCAI. \url{https://arxiv.org/abs/1505.04597}
  \item Streamlit Documentation: \url{https://docs.streamlit.io}
  \item PyTorch Documentation: \url{https://pytorch.org}
  \item Hugging Face Transformers: \url{https://huggingface.co/docs/transformers/index}
  \item Kaggle Dataset: \href{https://www.kaggle.com/datasets/humansintheloop/teeth-segmentation-on-dental-x-ray-images}{Teeth Segmentation Dataset on Kaggle}
\end{enumerate}

\end{document}
